{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build SQL database (Include all models: Figure and Text)\n",
    "This scripts create SQL file from the raw data in the `data` directories.  \n",
    "This scripts can be \"Run All\" if you don't need to see step-by-step results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"..\\data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read datasheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total models: 400\n",
      "Text models: 269\n",
      "Figure models: 131\n"
     ]
    }
   ],
   "source": [
    "df_loc = pd.read_csv(os.path.join(data_dir, 'Location.csv')) # The lat/lon should be pre-formatted in decimal units\n",
    "df_model_fig = pd.read_csv(os.path.join(data_dir, 'ModelAnalysis_Figure.csv'))\n",
    "df_model_text = pd.read_csv(os.path.join(data_dir, 'ModelAnalysis_Text.csv'))\n",
    "df_taxonomy = pd.read_csv(os.path.join(data_dir, 'ProcessHierarchyNetwork.csv'))\n",
    "df_FunctionType = pd.read_csv(os.path.join(data_dir, 'FunctionType.csv'))\n",
    "df_model_type = pd.read_csv(os.path.join(data_dir, 'ModelType.csv'))\n",
    "print(f\"Total models: {len(df_model_text)+len(df_model_fig)}\")\n",
    "print(f\"Text models: {len(df_model_text)}\")\n",
    "print(f\"Figure models: {len(df_model_fig)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_text['textmodel_snipped'] = df_model_text['textmodel_snipped'].str.replace('\"', \"'\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location table\n",
    "# Sanity check if data can be joined\n",
    "# df.set_index('key').join(other.set_index('key'))\n",
    "df_loc[\"id\"] = df_loc.index + 1\n",
    "df_loc[\"huc_watershed_id\"] = np.nan\n",
    "df_loc['long_name'] = df_loc['name'] + \", \" + df_loc['country']\n",
    "df_loc['lat'] = pd.to_numeric(df_loc['lat'], errors='coerce')\n",
    "df_loc['lon'] = pd.to_numeric(df_loc['lon'], errors='coerce')\n",
    "\n",
    "# FunctionType table\n",
    "df_FunctionType[\"id\"] = df_FunctionType.index + 1\n",
    "\n",
    "# Model type tale\n",
    "df_model_type[\"id\"] = df_model_type.index + 1\n",
    "\n",
    "# Alternative name table\n",
    "df_altNames0 = df_taxonomy.set_index(['process', 'function', 'identifier', 'process_level']).apply(\n",
    "    lambda x: x.str.split(',').explode()).reset_index()\n",
    "df_altNames = df_altNames0[['alternative_names', 'process']].copy()\n",
    "df_altNames['alternative_names'] = df_altNames['alternative_names'].str.strip()\n",
    "df_altNames['alternative_names'] = df_altNames['alternative_names'].str.capitalize()\n",
    "df_altNames.dropna(axis=0, inplace=True)\n",
    "df_altNames[\"id\"] = df_altNames.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all model dataframes into one \"df_model\"\n",
    "df_model_fig['model_type'] = 'Figure model'\n",
    "df_model_text['model_type']  = 'Text model'\n",
    "df_model = pd.concat([df_model_fig, df_model_text], join='outer', ignore_index=True).reset_index()\n",
    "df_model[\"id\"] = df_model.index + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "# Select columns that will be the main table in the database\n",
    "df_modelmain = df_model[['id', 'citation', 'model_type', 'watershed_name',\n",
    "                        'figure_num', 'figure_url', 'figure_caption',\n",
    "                        'textmodel_snipped', 'textmodel_section_number', 'textmodel_page_number', 'textmodel_section_name', \n",
    "                        'spatial_property', 'num_spatial_zones', 'temporal_property', \n",
    "                        'num_temporal_zones', 'vegetation_info', 'soil_info', 'geol_info',\n",
    "                        'topo_info', 'three_d_info', 'uncertainty_info', 'other_info'\n",
    "                        ]].copy()\n",
    "\n",
    "print(f'{len(df_modelmain)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelmain['spatial_property'].fillna('N', inplace=True)\n",
    "df_modelmain['temporal_property'].fillna('N', inplace=True)\n",
    "df_modelmain['figure_url'].fillna('N/A', inplace=True)\n",
    "df_modelmain['figure_caption'].fillna('N/A', inplace=True)\n",
    "df_modelmain['textmodel_section_number'].fillna('N/A', inplace=True)\n",
    "df_modelmain['textmodel_page_number'].fillna('N/A', inplace=True)\n",
    "df_modelmain['textmodel_section_name'].fillna('N/A', inplace=True)\n",
    "df_modelmain['vegetation_info'].fillna('N', inplace=True)\n",
    "df_modelmain['soil_info'].fillna('N', inplace=True)\n",
    "df_modelmain['geol_info'].fillna('N', inplace=True)\n",
    "df_modelmain['topo_info'].fillna('N', inplace=True)\n",
    "df_modelmain['three_d_info'].fillna('N', inplace=True)\n",
    "df_modelmain['uncertainty_info'].fillna('N', inplace=True)\n",
    "df_modelmain['other_info'].fillna('N', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping between 'attribution' values and URLs\n",
    "attributions_map = {\n",
    "    \"CC-BY-4.0\": \"https://creativecommons.org/licenses/by/4.0/\",\n",
    "    \"CC-BY 4.0\": \"https://creativecommons.org/licenses/by/4.0/\",\n",
    "    \"CC BY 4.0\": \"https://creativecommons.org/licenses/by/4.0/\",\n",
    "    \"CC-BY\": \"https://creativecommons.org/licenses/by/4.0/\",\n",
    "    \"CC-BY-3.0\": \"https://creativecommons.org/licenses/by/3.0/\",\n",
    "    \"CC-BY 3.0\": \"https://creativecommons.org/licenses/by/3.0/\",\n",
    "    \"CC BY 3.0\": \"https://creativecommons.org/licenses/by/3.0/\",\n",
    "    \"CC-BY-NC\": \"https://creativecommons.org/licenses/by-nc/4.0/\",\n",
    "    \"CC BY-NC\": \"https://creativecommons.org/licenses/by-nc/4.0/\",\n",
    "    \"CC BY NC\": \"https://creativecommons.org/licenses/by-nc/4.0/\",\n",
    "    \"CC-BY-NC-4.0\": \"https://creativecommons.org/licenses/by-nc/4.0/\",\n",
    "    \"CC BY-NC-4.0\": \"https://creativecommons.org/licenses/by-nc/4.0/\",\n",
    "    \"CC BY-NC 4.0\": \"https://creativecommons.org/licenses/by-nc/4.0/\",\n",
    "    \"CC-BY-NC 4.0\": \"https://creativecommons.org/licenses/by-nc/4.0/\",\n",
    "    \"CC BY NC 4.0\": \"https://creativecommons.org/licenses/by-nc/4.0/\",\n",
    "    \"CC-BY-NC-SA\": \"https://creativecommons.org/licenses/by-nc-sa/2.5/\",\n",
    "    \"CC BY-NC-SA\": \"https://creativecommons.org/licenses/by-nc-sa/2.5/\",\n",
    "    \"CC BY-NC-SA 2.5\": \"https://creativecommons.org/licenses/by-nc-sa/2.5/\",\n",
    "    \"CC-BY-NC-ND\": \"https://creativecommons.org/licenses/by-nc-nd/4.0/\",\n",
    "    \"CC BY-NC-ND\": \"https://creativecommons.org/licenses/by-nc-nd/4.0/\",\n",
    "    \"CC BY NC ND\": \"https://creativecommons.org/licenses/by-nc-nd/4.0/\"\n",
    "}\n",
    "\n",
    "# Apply the mapping to update 'attribution_url' column\n",
    "for attribution, url in attributions_map.items():\n",
    "    df_model.loc[df_model['attribution'] == attribution, \"attribution_url\"] = url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join citation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_model[\"attribution_url\"].fillna(df_model[\"url\"], inplace=True) # Fill the NaN in the attribution URL with paper URL (i.e., \"See paper for Not-open access ones\")\n",
    "df_citation = df_model[[\"citation\", \"url\"]].copy()\n",
    "df_citation[\"attribution\"] = df_model[\"attribution\"].copy()\n",
    "df_citation[\"attribution_url\"] = df_model[\"attribution_url\"].copy()\n",
    "df_citation[\"id\"] = df_citation.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join spatial and temporal zone tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spatialZoneType = df_model[\"spatial_property\"].copy().drop_duplicates()\n",
    "df_spatialZoneType = df_spatialZoneType.to_frame()\n",
    "df_spatialZoneType.reset_index(inplace=True)\n",
    "df_spatialZoneType = df_spatialZoneType.drop(columns='index')\n",
    "df_spatialZoneType['id'] = df_spatialZoneType.index + 1\n",
    "\n",
    "df_temporalZoneType = df_model[\"temporal_property\"].copy().drop_duplicates()\n",
    "df_temporalZoneType = df_temporalZoneType.to_frame()\n",
    "df_temporalZoneType.reset_index(inplace=True)\n",
    "df_temporalZoneType = df_temporalZoneType.drop(columns='index')\n",
    "df_temporalZoneType['id'] = df_temporalZoneType.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join LinkProcessPerceptual table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all the process original text and taxonomy name from model\n",
    "frames = [df_model[['id', 'flux1', 'flux1_taxonomy']].copy().rename(\n",
    "    columns={\"id\": \"entry_id\", \"flux1\": \"original_text\", \"flux1_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux2', 'flux2_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux2\": \"original_text\", \"flux2_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux3', 'flux3_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux3\": \"original_text\", \"flux3_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux4', 'flux4_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux4\": \"original_text\", \"flux4_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux5', 'flux5_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux5\": \"original_text\", \"flux5_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux6', 'flux6_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux6\": \"original_text\", \"flux6_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux7', 'flux7_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux7\": \"original_text\", \"flux7_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux8', 'flux8_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux8\": \"original_text\", \"flux8_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux9', 'flux9_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux9\": \"original_text\", \"flux9_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux10', 'flux10_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux10\": \"original_text\", \"flux10_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux11', 'flux11_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux11\": \"original_text\", \"flux11_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux12', 'flux12_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux12\": \"original_text\", \"flux12_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux13', 'flux13_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux13\": \"original_text\", \"flux13_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux14', 'flux14_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux14\": \"original_text\", \"flux14_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store1', 'store1_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store1\": \"original_text\", \"store1_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store2', 'store2_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store2\": \"original_text\", \"store2_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store3', 'store3_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store3\": \"original_text\", \"store3_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store4', 'store4_taxonomy']].copy().rename( columns={\"id\": \"entry_id\", \"store4\": \"original_text\", \"store4_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store5', 'store5_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store5\": \"original_text\", \"store5_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store6', 'store6_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store6\": \"original_text\", \"store6_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store7', 'store7_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store7\": \"original_text\", \"store7_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store8', 'store8_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store8\": \"original_text\", \"store8_taxonomy\": \"process\"}),\n",
    "        ]\n",
    "\n",
    "df_linkProcessPerceptual0 = pd.concat(frames, axis=0, ignore_index=True)\n",
    "df_linkProcessPerceptual0[\"id\"] = df_linkProcessPerceptual0.index + 1\n",
    "\n",
    "# Create taxonomy table\n",
    "df_process0 = df_taxonomy.drop(columns='alternative_names')\n",
    "\n",
    "# join process taxonomy and model table\n",
    "df_linkProcessPerceptual0[\"process_lower\"] = df_linkProcessPerceptual0['process'].str.lower()\n",
    "df_linkProcessPerceptual0[\"process_lower\"] = df_linkProcessPerceptual0['process_lower'].str.strip()\n",
    "df_process0[\"process_lower\"] = df_process0['process'].str.lower()\n",
    "df_process0[\"process_lower\"] = df_process0['process_lower'].str.strip()\n",
    "\n",
    "# find and add some new process from model table to taxonomy table (# Check here if you want to check process miscategorization)\n",
    "df_linkProcessPerceptual1 = df_linkProcessPerceptual0.merge(df_process0, on='process_lower', how='left')\n",
    "new_process = df_linkProcessPerceptual1.loc[(df_linkProcessPerceptual1['process_x'].isnull() == False) & (\n",
    "            df_linkProcessPerceptual1['process_y'].isnull() == True)]\n",
    "new_process.drop_duplicates(subset='process_lower', inplace=True)\n",
    "\n",
    "add_new_process = pd.DataFrame(\n",
    "    {'process': new_process['process_x'], 'process_lower': new_process['process_x'].str.lower(),\n",
    "     'identifier': ['NewProcess'] * len(new_process['process_x'])})\n",
    "df_process1 = pd.concat([df_process0, add_new_process])\n",
    "df_process1[\"id\"] = df_process1.reset_index().index + 1\n",
    "\n",
    "# re-join process taxonomy and model table with new process\n",
    "df_linkProcessPerceptual2 = df_linkProcessPerceptual0.merge(df_process1, on='process_lower', how='left')\n",
    "df_linkProcessPerceptual2.rename(columns={\"id_y\": \"process_id\"}, inplace=True)\n",
    "df_linkProcessPerceptual = df_linkProcessPerceptual2.drop(\n",
    "    columns={'process_x', 'id_x', 'process_lower', 'process_y', 'function', 'identifier', 'process_level'})\n",
    "df_linkProcessPerceptual.dropna(subset=['original_text'], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linkProcessPerceptual[\"id\"] = df_linkProcessPerceptual.reset_index().index + 1\n",
    "df_linkProcessPerceptual[\"process_id\"] = df_linkProcessPerceptual[\"process_id\"].astype('int') \n",
    "# If the above line returned an error, it's likely some entry is not finding matching taxonomy. Check the entry by running below\n",
    "# df_linkProcessPerceptual[df_linkProcessPerceptual['process_id'].isnull()]\n",
    "df_process = df_process1.drop(columns='process_lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(df_modelmain)}') # To check number of models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection to 'postgres'@'localhost' success!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.CursorResult at 0x226b0fe9760>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read config\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "DB_NAME = config['postgresql']['DB_NAME']\n",
    "HOST = config['postgresql']['HOST']\n",
    "PORT = config['postgresql']['PORT']\n",
    "USER_NAME = config['postgresql']['USER_NAME']\n",
    "PASSWD = config['postgresql']['PASSWD']\n",
    "schema = 'perceptual_model'\n",
    "\n",
    "# Connect to database\n",
    "conn_string = f'postgresql+psycopg2://{USER_NAME}:{PASSWD}@{HOST}:{PORT}/{DB_NAME}'\n",
    "db = create_engine(conn_string, client_encoding='utf8')\n",
    "try:\n",
    "    db_auto = db.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "    # This is added from SQL alchemy v2.0\n",
    "    # You have to COMMIT to put the results into database, and this options allows it\n",
    "    # See https://docs.sqlalchemy.org/en/20/core/connections.html\n",
    "    conn = db_auto.connect()\n",
    "    print(\"connection to '%s'@'%s' success!\" % (DB_NAME, HOST))\n",
    "except Exception as e:\n",
    "    print(\"connection to '%s'@'%s' failed.\" % (DB_NAME, HOST))\n",
    "    print(e)\n",
    "\n",
    "# Initial commands \n",
    "conn.execute(text(\"set search_path to public, perceptual_model\"))\n",
    "conn.execute(text(\"SET CLIENT_ENCODING TO 'UTF8';\"))\n",
    "\n",
    "# # View the records\n",
    "# results = conn.execute(text(f\"SELECT * from {schema}.locations\"))\n",
    "# for record in results:\n",
    "#     print(\"\\n\", record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped old tables (perceptual_model.locations)\n",
      "Dropped old tables (perceptual_model.citations)\n",
      "Dropped old tables (perceptual_model.spatial_zone_type)\n",
      "Dropped old tables (perceptual_model.temporal_zone_type)\n",
      "Dropped old tables (perceptual_model.process_alt_names)\n",
      "Dropped old tables (perceptual_model.function_type)\n",
      "Dropped old tables (perceptual_model.perceptual_model)\n",
      "Dropped old tables (perceptual_model.link_process_perceptual)\n",
      "Dropped old tables (perceptual_model.process_taxonomy)\n",
      "Dropped old tables (perceptual_model.model_type)\n"
     ]
    }
   ],
   "source": [
    "# Drop old tables\n",
    "tables = ['locations', 'citations', 'spatial_zone_type', 'temporal_zone_type', 'process_alt_names',\n",
    "            'function_type', 'perceptual_model', 'link_process_perceptual', 'process_taxonomy', 'model_type']\n",
    "for table in tables:\n",
    "    try:\n",
    "        conn.execute(text(f\"DROP TABLE {schema}.{table} CASCADE;\"))\n",
    "        print(f'Dropped old tables ({schema}.{table})')\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connarg = {'con': conn, 'schema': 'perceptual_model', 'if_exists': 'replace', 'index': False}\n",
    "\n",
    "# set table names in lower case https://github.com/pandas-dev/pandas/issues/13206\n",
    "df_loc.to_sql('locations', **connarg)\n",
    "df_citation.to_sql('citations', **connarg)\n",
    "df_spatialZoneType.to_sql('spatial_zone_type', **connarg)\n",
    "df_temporalZoneType.to_sql('temporal_zone_type', **connarg)\n",
    "df_altNames.to_sql('process_alt_names', **connarg)\n",
    "df_FunctionType.to_sql('function_type', **connarg)\n",
    "df_modelmain.to_sql('perceptual_model', **connarg)\n",
    "df_linkProcessPerceptual.to_sql('link_process_perceptual', **connarg)\n",
    "df_process.to_sql('process_taxonomy', **connarg)\n",
    "df_model_type.to_sql('model_type', **connarg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(psycopg2.errors.UndefinedObject) type \"geometry\" does not exist\n",
      "LINE 1: ...ER TABLE perceptual_model.locations ADD COLUMN pt geometry(p...\n",
      "                                                             ^\n",
      "\n",
      "[SQL: ALTER TABLE perceptual_model.locations ADD COLUMN pt geometry(point, 4326); WITH pt_geom AS ( \tSELECT id, ST_SetSRID(ST_MakePoint(lon, lat), 4326) AS pt \tfrom perceptual_model.locations \t) UPDATE perceptual_model.locations SET pt = pt_geom.pt FROM pt_geom WHERE pt_geom.id = locations.id;     ]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n"
     ]
    }
   ],
   "source": [
    "# Location lat lon --> geometry with PostGIS\n",
    "query = \"ALTER TABLE perceptual_model.locations \\\n",
    "ADD COLUMN pt geometry(point, 4326); \\\n",
    "WITH pt_geom AS ( \\\n",
    "\tSELECT id, ST_SetSRID(ST_MakePoint(lon, lat), 4326) AS pt \\\n",
    "\tfrom perceptual_model.locations \\\n",
    "\t) \\\n",
    "UPDATE perceptual_model.locations \\\n",
    "SET pt = pt_geom.pt \\\n",
    "FROM pt_geom \\\n",
    "WHERE pt_geom.id = locations.id; \\\n",
    "    \"\n",
    "\n",
    "try:\n",
    "    conn.execute(text(query))\n",
    "    print(\"success\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully added primary keys to table locations\n",
      "successfully added primary keys to table citations\n",
      "successfully added primary keys to table spatial_zone_type\n",
      "successfully added primary keys to table temporal_zone_type\n",
      "successfully added primary keys to table process_alt_names\n",
      "successfully added primary keys to table function_type\n",
      "successfully added primary keys to table perceptual_model\n",
      "successfully added primary keys to table link_process_perceptual\n",
      "successfully added primary keys to table process_taxonomy\n",
      "successfully added primary keys to table model_type\n"
     ]
    }
   ],
   "source": [
    "# Add primary keys\n",
    "for table in tables:\n",
    "    try:\n",
    "        conn.execute(text(f\"ALTER TABLE {schema}.{table} ADD PRIMARY KEY (id);\"))\n",
    "        print(f\"successfully added primary keys to table {table}\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "# add foreign keys\n",
    "\n",
    "# model & location\n",
    "query = \"ALTER TABLE perceptual_model ADD COLUMN location_id int; \\\n",
    "UPDATE perceptual_model \\\n",
    "SET location_id = locations.id \\\n",
    "FROM locations \\\n",
    "WHERE perceptual_model.watershed_name = locations.name; \\\n",
    "ALTER TABLE perceptual_model \\\n",
    "ADD FOREIGN KEY (location_id) REFERENCES locations(id); \\\n",
    "ALTER TABLE perceptual_model DROP COLUMN watershed_name;\"\n",
    "\n",
    "try:\n",
    "    conn.execute(text(query))\n",
    "    print(\"success\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "# model & citation\n",
    "query = \"ALTER TABLE perceptual_model ADD COLUMN citation_id int; \\\n",
    "UPDATE perceptual_model \\\n",
    "SET citation_id = citations.id \\\n",
    "FROM citations \\\n",
    "WHERE perceptual_model.citation = citations.citation; \\\n",
    "ALTER TABLE perceptual_model \\\n",
    "ADD FOREIGN KEY (citation_id) REFERENCES citations(id); \\\n",
    "ALTER TABLE perceptual_model DROP COLUMN citation;\"\n",
    "\n",
    "try:\n",
    "    conn.execute(text(query))\n",
    "    print(\"success\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "# model & spatial zone\n",
    "query = \"ALTER TABLE perceptual_model ADD COLUMN spatialzone_id int; \\\n",
    "UPDATE perceptual_model \\\n",
    "SET spatialzone_id = spatial_zone_type.id \\\n",
    "FROM spatial_zone_type \\\n",
    "WHERE perceptual_model.spatial_property = spatial_zone_type.spatial_property; \\\n",
    "ALTER TABLE perceptual_model \\\n",
    "ADD FOREIGN KEY (spatialzone_id) REFERENCES spatial_zone_type(id); \\\n",
    "ALTER TABLE perceptual_model DROP COLUMN spatial_property;\"\n",
    "\n",
    "try:\n",
    "    conn.execute(text(query))\n",
    "    print(\"success\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "\n",
    "# model & temporal zone\n",
    "query = \"ALTER TABLE perceptual_model ADD COLUMN temporalzone_id int; \\\n",
    "UPDATE perceptual_model \\\n",
    "SET temporalzone_id = temporal_zone_type.id \\\n",
    "FROM temporal_zone_type \\\n",
    "WHERE perceptual_model.temporal_property = temporal_zone_type.temporal_property; \\\n",
    "ALTER TABLE perceptual_model \\\n",
    "ADD FOREIGN KEY (temporalzone_id) REFERENCES temporal_zone_type(id); \\\n",
    "ALTER TABLE perceptual_model DROP COLUMN temporal_property;\"\n",
    "\n",
    "try:\n",
    "    conn.execute(text(query))\n",
    "    print(\"success\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# model & linktable\n",
    "query = \"ALTER TABLE link_process_perceptual  \\\n",
    "ADD FOREIGN KEY (entry_id) REFERENCES perceptual_model(id);\"\n",
    "\n",
    "try:\n",
    "    conn.execute(text(query))\n",
    "    print(\"success\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# process taxonomy & alternative names\n",
    "query = \"ALTER TABLE process_alt_names ADD COLUMN process_id int; \\\n",
    "UPDATE process_alt_names \\\n",
    "SET process_id = process_taxonomy.id \\\n",
    "FROM process_taxonomy \\\n",
    "WHERE process_alt_names.process = process_taxonomy.process; \\\n",
    "ALTER TABLE process_alt_names \\\n",
    "ADD FOREIGN KEY (process_id) REFERENCES process_taxonomy(id); \\\n",
    "ALTER TABLE process_alt_names DROP COLUMN process;\"\n",
    "\n",
    "try:\n",
    "    conn.execute(text(query))\n",
    "    print(\"success\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "\n",
    "# process taxonomy & function type\n",
    "query = \"ALTER TABLE process_taxonomy ADD COLUMN function_id int; \\\n",
    "UPDATE process_taxonomy \\\n",
    "SET function_id = function_type.id \\\n",
    "FROM function_type \\\n",
    "WHERE process_taxonomy.function = function_type.name; \\\n",
    "ALTER TABLE process_taxonomy \\\n",
    "ADD FOREIGN KEY (function_id) REFERENCES function_type(id); \\\n",
    "ALTER TABLE process_taxonomy DROP COLUMN function;\"\n",
    "\n",
    "try:\n",
    "    conn.execute(text(query))\n",
    "    print(\"success\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link model type & perceptual model -- success\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model & model type\n",
    "query = \"ALTER TABLE perceptual_model ADD COLUMN model_type_id int; \\\n",
    "UPDATE perceptual_model \\\n",
    "SET model_type_id = model_type.id \\\n",
    "FROM model_type \\\n",
    "WHERE perceptual_model.model_type = model_type.name; \\\n",
    "ALTER TABLE perceptual_model \\\n",
    "ADD FOREIGN KEY (model_type_id) REFERENCES model_type(id); \\\n",
    "ALTER TABLE perceptual_model DROP COLUMN model_type;\"\n",
    "\n",
    "try:\n",
    "    conn.execute(text(query))\n",
    "    print(\"Link model type & perceptual model -- success\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link taxonomy & perceptual model -- success\n"
     ]
    }
   ],
   "source": [
    "# try joining on pandas ...\n",
    "query = \"ALTER TABLE link_process_perceptual \\\n",
    "ADD FOREIGN KEY (process_id) REFERENCES process_taxonomy(id);\"\n",
    "# \\\n",
    "# ALTER TABLE link_process_perceptual DROP COLUMN process_name;\"\n",
    "\n",
    "try:\n",
    "    conn.execute(text(query))\n",
    "    print(\"Link taxonomy & perceptual model -- success\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only need to run if you want to debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join all tables and dump everything into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(psycopg2.errors.UndefinedTable) table \"giant_table\" does not exist\n",
      "\n",
      "[SQL: DROP TABLE giant_table;]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn.execute(text('DROP TABLE giant_table;'))\n",
    "    print(\"success\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = conn.execute(text('SELECT COUNT(*) FROM perceptual_model;'))\n",
    "    print(result.fetchone()[0])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create base giant table -- success\n"
     ]
    }
   ],
   "source": [
    "join_desired_tables = \"\\\n",
    "CREATE TEMP TABLE giant_table AS (\\\n",
    "SELECT perceptual_model.id, \\\n",
    "\t\tmodel_type.name AS model_type, \\\n",
    "\t\tcitations.citation, \\\n",
    "\t\tcitations.url, \\\n",
    "        citations.attribution, \\\n",
    "        citations.attribution_url, \\\n",
    "\t\tperceptual_model.figure_num, \\\n",
    "        perceptual_model.figure_caption, \\\n",
    "        perceptual_model.figure_url, \\\n",
    "\t\tperceptual_model.textmodel_snipped, \\\n",
    "        perceptual_model.textmodel_section_number, \\\n",
    "        perceptual_model.textmodel_section_name, \\\n",
    "\t\tperceptual_model.textmodel_page_number, \\\n",
    "        locations.long_name AS watershed_name, \\\n",
    "\t\tlocations.lat, \\\n",
    "\t\tlocations.lon, \\\n",
    "        locations.area_km2, \\\n",
    "\t\tprocess_taxonomy.process, \\\n",
    "\t\tprocess_taxonomy.identifier,\\\n",
    "\t\tfunction_type.name AS function_name,\\\n",
    "\t\tperceptual_model.num_spatial_zones,\\\n",
    "\t\tspatial_zone_type.spatial_property,\\\n",
    "\t\tperceptual_model.num_temporal_zones,\\\n",
    "\t\ttemporal_zone_type.temporal_property,\\\n",
    "\t\tperceptual_model.vegetation_info,\\\n",
    "\t\tperceptual_model.soil_info,\\\n",
    "\t\tperceptual_model.geol_info,\\\n",
    "\t\tperceptual_model.topo_info,\\\n",
    "\t\tperceptual_model.three_d_info,\\\n",
    "\t\tperceptual_model.uncertainty_info,\\\n",
    "\t\tperceptual_model.other_info\\\n",
    "\tFROM perceptual_model \\\n",
    "\tINNER JOIN citations \\\n",
    "\t\tON citations.id = perceptual_model.citation_id \\\n",
    "\tINNER JOIN locations \\\n",
    "\t\tON locations.id = perceptual_model.location_id \\\n",
    "\tINNER JOIN spatial_zone_type \\\n",
    "\t\tON spatial_zone_type.id = perceptual_model.spatialzone_id \\\n",
    "\tINNER JOIN temporal_zone_type \\\n",
    "\t\tON temporal_zone_type.id = perceptual_model.temporalzone_id \\\n",
    "\tINNER JOIN link_process_perceptual \\\n",
    "\t\tON perceptual_model.id = link_process_perceptual.entry_id \\\n",
    "\tINNER JOIN process_taxonomy \\\n",
    "\t\tON link_process_perceptual.process_id = process_taxonomy.id \\\n",
    "\tINNER JOIN function_type \\\n",
    "\t\tON process_taxonomy.function_id = function_type.id \\\n",
    "\tINNER JOIN model_type \\\n",
    "\t\tON perceptual_model.model_type_id = model_type.id \\\n",
    "ORDER BY perceptual_model.id \\\n",
    ");\"\n",
    "\n",
    "try:\n",
    "    conn.execute(text(join_desired_tables))\n",
    "    print(\"Create base giant table -- success\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = conn.execute(text('SELECT COUNT(*) FROM perceptual_model;'))\n",
    "    print(result.fetchone()[0])\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only need to run if you want to create csv file to create a webmap \n",
    "## Join all tables and output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(data_dir, \"for_arcgis_dashboard\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch results from giant tables -- success\n",
      "Index(['id', 'figure_num', 'figure_url', 'figure_caption', 'textmodel_snipped',\n",
      "       'textmodel_section_number', 'textmodel_page_number',\n",
      "       'textmodel_section_name', 'num_spatial_zones', 'num_temporal_zones',\n",
      "       'vegetation_info', 'soil_info', 'geol_info', 'topo_info',\n",
      "       'three_d_info', 'uncertainty_info', 'other_info', 'location_id',\n",
      "       'citation_id', 'spatialzone_id', 'temporalzone_id', 'model_type_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT * FROM perceptual_model\"\n",
    "try:\n",
    "    df_results = pd.read_sql(text(query), conn)\n",
    "    # results = conn.execute(query).fetchall()\n",
    "    print(\"Fetch results from giant tables -- success\")\n",
    "    print(df_results.columns)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "df_results.to_csv(os.path.join(output_dir, 'giant_table_debug.csv'), sep=',', header=True, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create giant model table with flux & stores -- success\n"
     ]
    }
   ],
   "source": [
    "join_desired_tables2 = \"\\\n",
    "CREATE TEMP TABLE giant_table_base AS ( \\\n",
    "SELECT DISTINCT \\\n",
    "    id,  \\\n",
    "\tmodel_type,\\\n",
    "\tcitation,  \\\n",
    "\turl,  \\\n",
    "\tattribution,  \\\n",
    "\tattribution_url,  \\\n",
    "\tfigure_num,  \\\n",
    "\tfigure_caption,  \\\n",
    "\tfigure_url,  \\\n",
    "\ttextmodel_snipped, \\\n",
    "\ttextmodel_section_number, \\\n",
    "\ttextmodel_section_name, \\\n",
    "\ttextmodel_page_number, \\\n",
    "\twatershed_name,  \\\n",
    "\tlat,  \\\n",
    "\tlon,  \\\n",
    "\tarea_km2,  \\\n",
    "\tnum_spatial_zones,  \\\n",
    "\tspatial_property,  \\\n",
    "\tnum_temporal_zones,  \\\n",
    "\ttemporal_property,  \\\n",
    "\tvegetation_info,  \\\n",
    "\tsoil_info,  \\\n",
    "\tgeol_info,  \\\n",
    "\ttopo_info,  \\\n",
    "\tthree_d_info,  \\\n",
    "\tuncertainty_info,  \\\n",
    "\tother_info \\\n",
    "FROM giant_table \\\n",
    "); \\\n",
    "CREATE TEMP TABLE giant_table_flux AS (  \\\n",
    "SELECT DISTINCT  \\\n",
    "id AS temp_id1,  \\\n",
    "COUNT(process) OVER(PARTITION BY id) AS num_flux,  \\\n",
    "STRING_AGG(process, ', ') OVER(PARTITION BY id) AS flux_list,  \\\n",
    "STRING_AGG(identifier, ', ') OVER(PARTITION BY id) AS flux_id_list  \\\n",
    "FROM   giant_table  \\\n",
    "\tWHERE function_name ILIKE 'Filling of store'  \\\n",
    "    OR function_name IS NULL\\\n",
    "\tOR function_name ILIKE 'Release from store'  \\\n",
    "\tOR function_name ILIKE 'In-catchment flux'  \\\n",
    "\tOR function_name ILIKE 'In-store flux'  \\\n",
    "\tOR function_name ILIKE 'Release'  \\\n",
    "\t); \\\n",
    "CREATE TEMP TABLE giant_table_store AS ( \\\n",
    "SELECT DISTINCT id AS temp_id2,  \\\n",
    "COUNT(process) OVER(PARTITION BY id) AS num_store,  \\\n",
    "STRING_AGG(process, ', ') OVER(PARTITION BY id) AS store_list,  \\\n",
    "STRING_AGG(identifier, ', ') OVER(PARTITION BY id) AS store_id_list  \\\n",
    "FROM giant_table  \\\n",
    "\tWHERE function_name ILIKE 'Store'  \\\n",
    "\tOR function_name ILIKE 'Store, temporary'  \\\n",
    "\tOR function_name ILIKE 'Store characteristics, temporary'  \\\n",
    "\tOR function_name ILIKE 'Store characteristics, permanent'  \\\n",
    "\t);  \\\n",
    "CREATE TEMP TABLE giant_table_update AS ( \\\n",
    "SELECT * FROM giant_table_base  \\\n",
    "LEFT JOIN giant_table_store  \\\n",
    "ON giant_table_base.id = giant_table_store.temp_id2 \\\n",
    "LEFT JOIN giant_table_flux \\\n",
    "ON giant_table_base.id = giant_table_flux.temp_id1 \\\n",
    "ORDER BY id  \\\n",
    "\t); \\\n",
    "ALTER TABLE giant_table_update \\\n",
    "DROP COLUMN temp_id1, \\\n",
    "DROP COLUMN temp_id2,  \\\n",
    "ADD COLUMN dummy_column NUMERIC DEFAULT 1; \\\n",
    "\"\n",
    "\n",
    "try:\n",
    "    conn.execute(text(join_desired_tables2))\n",
    "    print(\"Create giant model table with flux & stores -- success\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = conn.execute(text('SELECT COUNT(*) FROM giant_table_base;'))\n",
    "    print(result.fetchone()[0])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = conn.execute(text('SELECT giant_table.id FROM giant_table LEFT OUTER JOIN giant_table_update ON giant_table.id = giant_table_update.id WHERE giant_table_update.id IS NULL;'))\n",
    "    print(result.fetchone()[0])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch results from giant tables -- success\n",
      "Index(['id', 'model_type', 'citation', 'url', 'attribution', 'attribution_url',\n",
      "       'figure_num', 'figure_caption', 'figure_url', 'textmodel_snipped',\n",
      "       'textmodel_section_number', 'textmodel_section_name',\n",
      "       'textmodel_page_number', 'watershed_name', 'lat', 'lon', 'area_km2',\n",
      "       'num_spatial_zones', 'spatial_property', 'num_temporal_zones',\n",
      "       'temporal_property', 'vegetation_info', 'soil_info', 'geol_info',\n",
      "       'topo_info', 'three_d_info', 'uncertainty_info', 'other_info',\n",
      "       'num_store', 'store_list', 'store_id_list', 'num_flux', 'flux_list',\n",
      "       'flux_id_list', 'dummy_column'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT * FROM giant_table_update\"\n",
    "try:\n",
    "    df_results = pd.read_sql(text(query), conn)\n",
    "    # results = conn.execute(query).fetchall()\n",
    "    print(\"Fetch results from giant tables -- success\")\n",
    "    print(df_results.columns)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = conn.execute(text('SELECT COUNT(*) FROM giant_table_update;'))\n",
    "    print(result.fetchone()[0])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_name = \"giant_table\"\n",
    "# Some last housekeeping\n",
    "df_results['huc_watershed_id'] = 'N/A'\n",
    "df_results['num_store'].fillna(0, inplace=True)\n",
    "df_results['num_flux'].fillna(0, inplace=True)\n",
    "df_results['area_km2'].fillna(-9999, inplace=True)\n",
    "df_results.fillna('N/A', inplace=True)\n",
    "df_results.to_csv(os.path.join(output_dir, f'{out_file_name}.csv'), sep=',', header=True, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported results to ../data/giant_table.csv:\n",
      "2024-12-26 14:06:38.676754\n",
      "Number of models: 400\n"
     ]
    }
   ],
   "source": [
    "conn.close()\n",
    "db.dispose()\n",
    "print(fr'Exported results to ../data/{out_file_name}.csv:')\n",
    "\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "print(f\"Number of models: {len(df_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3140ae756112501f18b23ec79a6bbbb54e715b030c96600dfefb60d12a8f240f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
